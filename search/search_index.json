{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Opti The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved. Experimental design In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. equality: \\(\\sum x_i = 1\\) inequality: \\(2 x_1 < x_2\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. Multiobjective optimization In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision. Bayesian optimization In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Opti"},{"location":"#opti","text":"The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.","title":"Opti"},{"location":"#experimental-design","text":"In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. equality: \\(\\sum x_i = 1\\) inequality: \\(2 x_1 < x_2\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.","title":"Experimental design"},{"location":"#multiobjective-optimization","text":"In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.","title":"Multiobjective optimization"},{"location":"#bayesian-optimization","text":"In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Bayesian optimization"},{"location":"install/","text":"Install Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"install/#install","text":"Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"overview/","text":"Overview Opti problems consist of a definition of the input space \\(x \\in \\mathbb{X}\\) , output space \\(y \\in \\mathbb{Y}\\) , objectives \\(s(y)\\) , constraints \\(g(x) \\leq 0\\) , output constraints \\(h(y)\\) and possibly an existing data set. Parameters Input and output spaces are defined using the Parameters class, for example from opti.parameter import Parameters , Continuous , Discrete , Categorical inputs = Parameters ([ Continuous ( \"x1\" , domain = [ 0 , 1 ]), Continuous ( \"x2\" , domain = [ 0 , 1 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = Parameters ([ Continuous ( \"y1\" , domain = [ 0 , None ]), Continuous ( \"y2\" , domain = [ None , None ]), Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ \"A\" , \"C\" , \"A\" , \"C\" , \"A\" ], dtype = object ) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A and check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) Note that in opti all functions operating on dataframes use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe. Constraints Input constraints are defined separately from the input space. There are currently five supported types of constraints. from opti import Constraints , LinearEquality , LinearInequality , NonlinearEquality , NonlinearInequality , NChooseK Linear constraints are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there's a constraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take non-zero values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 parameters can be non-zero constr5 = NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) As with the parameters there's a container which acts as the union of a list of multiple constraints. constraints = Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point is satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 . eval ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to sample under constraints, see the sampling reference. Objectives In an optimization problem we want to be able to define the target direction or target value individually for each output. This is done using objectives from opti.objective import Objectives , Minimize , Maximize , CloseToTarget objectives = Objectives ([ Minimize ( \"y1\" ), Maximize ( \"y2\" ), CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives . eval ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs. Problem Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Overview"},{"location":"overview/#overview","text":"Opti problems consist of a definition of the input space \\(x \\in \\mathbb{X}\\) , output space \\(y \\in \\mathbb{Y}\\) , objectives \\(s(y)\\) , constraints \\(g(x) \\leq 0\\) , output constraints \\(h(y)\\) and possibly an existing data set.","title":"Overview"},{"location":"overview/#parameters","text":"Input and output spaces are defined using the Parameters class, for example from opti.parameter import Parameters , Continuous , Discrete , Categorical inputs = Parameters ([ Continuous ( \"x1\" , domain = [ 0 , 1 ]), Continuous ( \"x2\" , domain = [ 0 , 1 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = Parameters ([ Continuous ( \"y1\" , domain = [ 0 , None ]), Continuous ( \"y2\" , domain = [ None , None ]), Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ \"A\" , \"C\" , \"A\" , \"C\" , \"A\" ], dtype = object ) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A and check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) Note that in opti all functions operating on dataframes use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe.","title":"Parameters"},{"location":"overview/#constraints","text":"Input constraints are defined separately from the input space. There are currently five supported types of constraints. from opti import Constraints , LinearEquality , LinearInequality , NonlinearEquality , NonlinearInequality , NChooseK Linear constraints are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there's a constraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take non-zero values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 parameters can be non-zero constr5 = NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) As with the parameters there's a container which acts as the union of a list of multiple constraints. constraints = Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point is satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 . eval ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to sample under constraints, see the sampling reference.","title":"Constraints"},{"location":"overview/#objectives","text":"In an optimization problem we want to be able to define the target direction or target value individually for each output. This is done using objectives from opti.objective import Objectives , Minimize , Maximize , CloseToTarget objectives = Objectives ([ Minimize ( \"y1\" ), Maximize ( \"y2\" ), CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives . eval ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs.","title":"Objectives"},{"location":"overview/#problem","text":"Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Problem"},{"location":"ref-constraint/","text":"Constraints Constraint Base class to define constraints on the input space, g(x) = 0 or g(x) <= 0. eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint. Args: data: Data to evaluate the constraint on. Returns: Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. \"\"\" raise NotImplementedError satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied. Args: data: Data to evaluate the constraint on. Returns: Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. \"\"\" raise NotImplementedError Constraints List of optimization constraints eval ( self , data ) Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c . eval ( data ) for c in self . constraints ], axis = 1 ) satisfied ( self , data ) Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) LinearEquality ( Constraint ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , np . ndarray ] = 1 , rhs : float = 0 ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self . eval ( data ), 0 ), index = data . index ) LinearInequality ( Constraint ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , np . ndarray ] = 1 , rhs : float = 0 ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self . eval ( data ) <= 0 NChooseK ( Constraint ) __init__ ( self , names , max_active ) special Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : x = data [ self . names ] . values num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index ) satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self . eval ( data ) <= 0 , index = data . index ) NonlinearEquality ( Constraint ) __init__ ( self , expression ) special Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self . eval ( data ), 0 ), index = data . index ) NonlinearInequality ( Constraint ) __init__ ( self , expression ) special Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False eval ( self , data ) Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) satisfied ( self , data ) Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self . eval ( data ) <= 0","title":"Constraint"},{"location":"ref-constraint/#constraints","text":"","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraint","text":"Base class to define constraints on the input space, g(x) = 0 or g(x) <= 0.","title":"Constraint"},{"location":"ref-constraint/#opti.constraint.Constraint.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint. Args: data: Data to evaluate the constraint on. Returns: Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. \"\"\" raise NotImplementedError","title":"eval()"},{"location":"ref-constraint/#opti.constraint.Constraint.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied. Args: data: Data to evaluate the constraint on. Returns: Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. \"\"\" raise NotImplementedError","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.Constraints","text":"List of optimization constraints","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraints.eval","text":"Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c . eval ( data ) for c in self . constraints ], axis = 1 )","title":"eval()"},{"location":"ref-constraint/#opti.constraint.Constraints.satisfied","text":"Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearEquality","text":"","title":"LinearEquality"},{"location":"ref-constraint/#opti.constraint.LinearEquality.__init__","text":"Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , np . ndarray ] = 1 , rhs : float = 0 ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearEquality.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs","title":"eval()"},{"location":"ref-constraint/#opti.constraint.LinearEquality.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self . eval ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearInequality","text":"","title":"LinearInequality"},{"location":"ref-constraint/#opti.constraint.LinearInequality.__init__","text":"Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , np . ndarray ] = 1 , rhs : float = 0 ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearInequality.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs","title":"eval()"},{"location":"ref-constraint/#opti.constraint.LinearInequality.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self . eval ( data ) <= 0","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NChooseK","text":"","title":"NChooseK"},{"location":"ref-constraint/#opti.constraint.NChooseK.__init__","text":"Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NChooseK.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : x = data [ self . names ] . values num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index )","title":"eval()"},{"location":"ref-constraint/#opti.constraint.NChooseK.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self . eval ( data ) <= 0 , index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality","text":"","title":"NonlinearEquality"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.__init__","text":"Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression )","title":"eval()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self . eval ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality","text":"","title":"NonlinearInequality"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.__init__","text":"Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.eval","text":"Numerically evaluate the constraint. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) with equalities interpreted as g(x) = 0 and inequalities as g(x) <=0. Source code in opti/constraint.py def eval ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression )","title":"eval()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.satisfied","text":"Check if a constraint is satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraint on. required Returns: Type Description Series Constraint evaluation g(x) = 0 or g(x) <= 0 depending on the constraint type. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self . eval ( data ) <= 0","title":"satisfied()"},{"location":"ref-metric/","text":"Metrics crowding_distance ( A ) Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. !!! reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 ) generational_distance ( A , R , p = 1 , clip = True ) Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. !!! reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A ) inverted_generational_distance ( A , R , p = 1 ) Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. !!! reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False ) is_pareto_efficient ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient pareto_front ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"Metric"},{"location":"ref-metric/#metrics","text":"","title":"Metrics"},{"location":"ref-metric/#opti.metric.crowding_distance","text":"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. !!! reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 )","title":"crowding_distance()"},{"location":"ref-metric/#opti.metric.generational_distance","text":"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. !!! reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A )","title":"generational_distance()"},{"location":"ref-metric/#opti.metric.inverted_generational_distance","text":"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. !!! reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False )","title":"inverted_generational_distance()"},{"location":"ref-metric/#opti.metric.is_pareto_efficient","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient","title":"is_pareto_efficient()"},{"location":"ref-metric/#opti.metric.pareto_front","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"pareto_front()"},{"location":"ref-model/","text":"Models LinearModel ( Model ) eval ( self , x ) Evaluate the objective values for a given DataFrame. Source code in opti/model.py def eval ( self , x : pd . DataFrame ) -> pd . DataFrame : y = x @ self . coefficients + self . offset return pd . DataFrame ( y , columns = self . names ) to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , ) Model __init__ ( self , names ) special Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names ) eval ( self , df ) Evaluate the objective values for a given DataFrame. Source code in opti/model.py def eval ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error Models Container for models.","title":"Model"},{"location":"ref-model/#models","text":"","title":"Models"},{"location":"ref-model/#opti.model.LinearModel","text":"","title":"LinearModel"},{"location":"ref-model/#opti.model.LinearModel.eval","text":"Evaluate the objective values for a given DataFrame. Source code in opti/model.py def eval ( self , x : pd . DataFrame ) -> pd . DataFrame : y = x @ self . coefficients + self . offset return pd . DataFrame ( y , columns = self . names )","title":"eval()"},{"location":"ref-model/#opti.model.LinearModel.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , )","title":"to_config()"},{"location":"ref-model/#opti.model.Model","text":"","title":"Model"},{"location":"ref-model/#opti.model.Model.__init__","text":"Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names )","title":"__init__()"},{"location":"ref-model/#opti.model.Model.eval","text":"Evaluate the objective values for a given DataFrame. Source code in opti/model.py def eval ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError","title":"eval()"},{"location":"ref-model/#opti.model.Model.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error","title":"to_config()"},{"location":"ref-model/#opti.model.Models","text":"Container for models.","title":"Models"},{"location":"ref-objective/","text":"Objectives CloseToTarget ( Objective ) __init__ ( self , parameter , target = 0 , exponent = 1 , tolerance = 0 ) special Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default parameter str parameter to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: parameter: parameter to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target self . exponent = exponent self . tolerance = tolerance to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config Maximize ( Objective ) __init__ ( self , parameter , target = 0 ) special Maximization objective s(y) = target - y Parameters: Name Type Description Default name name of the objective (= parameter to optimize) required target float value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: name of the objective (= parameter to optimize) target: value above which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Undo the transformation from output to objective value\"\"\" return - y - self . target Minimize ( Objective ) __init__ ( self , parameter , target = 0 ) special Minimization objective s(y) = y - target Parameters: Name Type Description Default parameter str parameter to minimize required target float value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: parameter: parameter to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = self . target return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target Objective __call__ ( self , y ) special Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError # implemented in the derived classes __init__ ( self , name , parameter ) special Base class for optimzation objectives. Parameters: Name Type Description Default name str name of the objective required parameters parameter(s) that the objective is operating on required Source code in opti/objective.py def __init__ ( self , name : str , parameter : Union [ str , List [ str ]]): \"\"\"Base class for optimzation objectives. Args: name: name of the objective parameters: parameter(s) that the objective is operating on \"\"\" self . name = name self . parameter = parameter eval ( self , df ) Evaluate the objective values for a given DataFrame. Source code in opti/objective.py def eval ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" return pd . Series ( self ( df [ self . parameter ] . values ), index = df . index , name = self . name ) to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError # implemented in the derived classes Objectives Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) bounds ( self , outputs ) Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self . eval ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds make_objective ( type , name , ** kwargs ) Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name Union[str, List[str]] parameter(s) that the objective is operating on required Source code in opti/objective.py def make_objective ( type : str , name : Union [ str , List [ str ]], ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: parameter(s) that the objective is operating on \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( parameter = name , ** kwargs )","title":"Objective"},{"location":"ref-objective/#objectives","text":"","title":"Objectives"},{"location":"ref-objective/#opti.objective.CloseToTarget","text":"","title":"CloseToTarget"},{"location":"ref-objective/#opti.objective.CloseToTarget.__init__","text":"Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default parameter str parameter to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: parameter: parameter to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target self . exponent = exponent self . tolerance = tolerance","title":"__init__()"},{"location":"ref-objective/#opti.objective.CloseToTarget.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize","text":"","title":"Maximize"},{"location":"ref-objective/#opti.objective.Maximize.__init__","text":"Maximization objective s(y) = target - y Parameters: Name Type Description Default name name of the objective (= parameter to optimize) required target float value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: name of the objective (= parameter to optimize) target: value above which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Maximize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Undo the transformation from output to objective value\"\"\" return - y - self . target","title":"untransform()"},{"location":"ref-objective/#opti.objective.Minimize","text":"","title":"Minimize"},{"location":"ref-objective/#opti.objective.Minimize.__init__","text":"Minimization objective s(y) = y - target Parameters: Name Type Description Default parameter str parameter to minimize required target float value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , parameter : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: parameter: parameter to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name = parameter , parameter = parameter ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Minimize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . parameter , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = self . target return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Minimize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target","title":"untransform()"},{"location":"ref-objective/#opti.objective.Objective","text":"","title":"Objective"},{"location":"ref-objective/#opti.objective.Objective.__call__","text":"Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , y : np . ndarray ) -> np . ndarray : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError # implemented in the derived classes","title":"__call__()"},{"location":"ref-objective/#opti.objective.Objective.__init__","text":"Base class for optimzation objectives. Parameters: Name Type Description Default name str name of the objective required parameters parameter(s) that the objective is operating on required Source code in opti/objective.py def __init__ ( self , name : str , parameter : Union [ str , List [ str ]]): \"\"\"Base class for optimzation objectives. Args: name: name of the objective parameters: parameter(s) that the objective is operating on \"\"\" self . name = name self . parameter = parameter","title":"__init__()"},{"location":"ref-objective/#opti.objective.Objective.eval","text":"Evaluate the objective values for a given DataFrame. Source code in opti/objective.py def eval ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" return pd . Series ( self ( df [ self . parameter ] . values ), index = df . index , name = self . name )","title":"eval()"},{"location":"ref-objective/#opti.objective.Objective.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError # implemented in the derived classes","title":"to_config()"},{"location":"ref-objective/#opti.objective.Objectives","text":"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint)","title":"Objectives"},{"location":"ref-objective/#opti.objective.Objectives.bounds","text":"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self . eval ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds","title":"bounds()"},{"location":"ref-objective/#opti.objective.make_objective","text":"Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name Union[str, List[str]] parameter(s) that the objective is operating on required Source code in opti/objective.py def make_objective ( type : str , name : Union [ str , List [ str ]], ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: parameter(s) that the objective is operating on \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( parameter = name , ** kwargs )","title":"make_objective()"},{"location":"ref-parameter/","text":"Parameters Categorical ( Parameter ) Categorical parameter (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) from_label_encoding ( self , points ) Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index ) from_onehot_encoding ( self , points ) Convert points brack from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points brack from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_label_encoding ( self , points ) Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s to_onehot_encoding ( self , points ) Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) Continuous ( Parameter ) Parameter that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound] bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) from_unit_range ( self , points ) Transform points from the unit range: [0-1] -> [low, high]. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable configuration dict.\"\"\" low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) conf = dict ( name = self . name , type = self . type , domain = [ low , high ]) conf . update ( self . extra_fields ) return conf to_unit_range ( self , points ) Transform points to the unit range: [low, high] -> [0, 1]. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Discrete ( Parameter ) Discrete parameter (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) is_integer ( self ) Check if the domain is an integer range, such as [1, 2, 3] Source code in opti/parameter.py def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_unit_range ( self , points ) Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Parameter Parameter base class. to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf Parameters Set of parameters representing either the input or the output space. bounds : DataFrame property readonly Return the parameter bounds. __init__ ( self , parameters ) special It can be constructed either from a list / tuple (of at least one) Parameter objects Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d contains ( self , points ) Check if points are inside the space in each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis =- 1 ) return b . all ( axis =- 1 ) round ( self , points ) Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) sample ( self , n = 1 ) Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) to_config ( self ) Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ Dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()] make_parameter ( name , type , domain = None , ** kwargs ) Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"domain not given for parameter { name } \" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"Parameter"},{"location":"ref-parameter/#parameters","text":"","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Categorical","text":"Categorical parameter (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values","title":"Categorical"},{"location":"ref-parameter/#opti.parameter.Categorical.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_label_encoding","text":"Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index )","title":"from_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_onehot_encoding","text":"Convert points brack from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points brack from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s","title":"from_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point","title":"round()"},{"location":"ref-parameter/#opti.parameter.Categorical.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_label_encoding","text":"Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s","title":"to_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_onehot_encoding","text":"Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float )","title":"to_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Continuous","text":"Parameter that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound]","title":"Continuous"},{"location":"ref-parameter/#opti.parameter.Continuous.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Continuous.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Continuous.from_unit_range","text":"Transform points from the unit range: [0-1] -> [low, high]. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low","title":"from_unit_range()"},{"location":"ref-parameter/#opti.parameter.Continuous.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Continuous.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable configuration dict.\"\"\" low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) conf = dict ( name = self . name , type = self . type , domain = [ low , high ]) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_unit_range","text":"Transform points to the unit range: [low, high] -> [0, 1]. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Discrete","text":"Discrete parameter (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values","title":"Discrete"},{"location":"ref-parameter/#opti.parameter.Discrete.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Discrete.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Discrete.is_integer","text":"Check if the domain is an integer range, such as [1, 2, 3] Source code in opti/parameter.py def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False","title":"is_integer()"},{"location":"ref-parameter/#opti.parameter.Discrete.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Discrete.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Discrete.to_unit_range","text":"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Parameter","text":"Parameter base class.","title":"Parameter"},{"location":"ref-parameter/#opti.parameter.Parameter.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Parameters","text":"Set of parameters representing either the input or the output space.","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Parameters.bounds","text":"Return the parameter bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Parameters.__init__","text":"It can be constructed either from a list / tuple (of at least one) Parameter objects Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d","title":"__init__()"},{"location":"ref-parameter/#opti.parameter.Parameters.contains","text":"Check if points are inside the space in each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis =- 1 ) return b . all ( axis =- 1 )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Parameters.round","text":"Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Parameters.sample","text":"Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 )","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Parameters.to_config","text":"Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ Dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()]","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.make_parameter","text":"Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"domain not given for parameter { name } \" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"make_parameter()"},{"location":"ref-problem/","text":"Problem Problem __init__ ( self , inputs , outputs , objectives = None , constraints = None , output_constraints = None , f = None , models = None , data = None , optima = None , name = None ) special An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Signature: f(x: np.ndarray) -> np.ndarray where both x and f(x) are 2D arrays for vectorized evaluation. None data Optional[pandas.core.frame.DataFrame] Experimental data. None optima Optional[pandas.core.frame.DataFrame] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ pd . DataFrame ] = None , optima : Optional [ pd . DataFrame ] = None , name : Optional [ str ] = None , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Signature: f(x: np.ndarray) -> np.ndarray where both x and f(x) are 2D arrays for vectorized evaluation. data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f # checks self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () add_data ( self , data ) Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) check_data ( self , data ) Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" ) check_models ( self ) Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) check_problem ( self ) Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check if inputs and outputs are consistent duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if objectives are consistent all_parameters = self . inputs . names + self . outputs . names for obj in self . objectives : p = obj . parameter if p not in all_parameters : raise ValueError ( f \"Objective refers to unknown parameter: { p } \" ) create_initial_data ( self , n_samples = 10 ) Create an initial data set by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. \"\"\" X = self . sample_inputs ( n_samples ) self . data = pd . concat ([ X , self . eval ( X )], axis = 1 ) eval ( self , data ) Evaluate the function on a DataFrame Source code in opti/problem.py def eval ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the function on a DataFrame\"\"\" x = data [ self . inputs . names ] . values y = self . f ( x ) if y . ndim == 1 : y = y [:, None ] return pd . DataFrame ( y , index = data . index , columns = self . outputs . names ) from_config ( config ) staticmethod Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" outputs = Parameters ( config [ \"outputs\" ]) objectives = config . get ( \"objectives\" , None ) if objectives is not None : objectives = [ make_objective ( ** o ) for o in config [ \"objectives\" ]] else : objectives = Objectives ([ Minimize ( d . name ) for d in outputs ]) output_constraints = config . get ( \"output_constraints\" , None ) if output_constraints is not None : output_constraints = [ make_objective ( ** o ) for o in config [ \"output_constraints\" ] ] models = config . get ( \"models\" , None ) if models is not None : models = Models ( models ) data = config . get ( \"data\" , None ) if data : data = pd . DataFrame ( ** config [ \"data\" ]) optima = config . get ( \"optima\" , None ) if optima : optima = pd . DataFrame ( ** config [ \"optima\" ]) return Problem ( inputs = config [ \"inputs\" ], outputs = outputs , objectives = objectives , constraints = config . get ( \"constraints\" , None ), output_constraints = output_constraints , models = models , data = data , optima = optima , name = config . get ( \"name\" , None ), ) from_json ( fname ) staticmethod Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem . from_config ( config ) get_X ( self , data = None ) Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values get_XY ( self , outputs = None , data = None , continuous = 'none' , discrete = 'none' , categorical = 'none' ) Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y get_X_bounds ( self ) Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi get_Y ( self , data = None ) Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values get_data ( self ) Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data sample_inputs ( self , n_samples = 10 ) Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) set_data ( self , data ) Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data set_optima ( self , optima ) Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima to_config ( self ) Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> Dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config to_json ( self , fname ) Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , indent = 2 ) outfile . write ( b . encode ( \"utf-8\" )) read_json ( filepath ) Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"Problem"},{"location":"ref-problem/#problem","text":"","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem","text":"","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem.__init__","text":"An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Signature: f(x: np.ndarray) -> np.ndarray where both x and f(x) are 2D arrays for vectorized evaluation. None data Optional[pandas.core.frame.DataFrame] Experimental data. None optima Optional[pandas.core.frame.DataFrame] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ pd . DataFrame ] = None , optima : Optional [ pd . DataFrame ] = None , name : Optional [ str ] = None , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Signature: f(x: np.ndarray) -> np.ndarray where both x and f(x) are 2D arrays for vectorized evaluation. data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f # checks self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models ()","title":"__init__()"},{"location":"ref-problem/#opti.problem.Problem.add_data","text":"Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 )","title":"add_data()"},{"location":"ref-problem/#opti.problem.Problem.check_data","text":"Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" )","title":"check_data()"},{"location":"ref-problem/#opti.problem.Problem.check_models","text":"Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" )","title":"check_models()"},{"location":"ref-problem/#opti.problem.Problem.check_problem","text":"Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check if inputs and outputs are consistent duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if objectives are consistent all_parameters = self . inputs . names + self . outputs . names for obj in self . objectives : p = obj . parameter if p not in all_parameters : raise ValueError ( f \"Objective refers to unknown parameter: { p } \" )","title":"check_problem()"},{"location":"ref-problem/#opti.problem.Problem.create_initial_data","text":"Create an initial data set by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. \"\"\" X = self . sample_inputs ( n_samples ) self . data = pd . concat ([ X , self . eval ( X )], axis = 1 )","title":"create_initial_data()"},{"location":"ref-problem/#opti.problem.Problem.eval","text":"Evaluate the function on a DataFrame Source code in opti/problem.py def eval ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the function on a DataFrame\"\"\" x = data [ self . inputs . names ] . values y = self . f ( x ) if y . ndim == 1 : y = y [:, None ] return pd . DataFrame ( y , index = data . index , columns = self . outputs . names )","title":"eval()"},{"location":"ref-problem/#opti.problem.Problem.from_config","text":"Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" outputs = Parameters ( config [ \"outputs\" ]) objectives = config . get ( \"objectives\" , None ) if objectives is not None : objectives = [ make_objective ( ** o ) for o in config [ \"objectives\" ]] else : objectives = Objectives ([ Minimize ( d . name ) for d in outputs ]) output_constraints = config . get ( \"output_constraints\" , None ) if output_constraints is not None : output_constraints = [ make_objective ( ** o ) for o in config [ \"output_constraints\" ] ] models = config . get ( \"models\" , None ) if models is not None : models = Models ( models ) data = config . get ( \"data\" , None ) if data : data = pd . DataFrame ( ** config [ \"data\" ]) optima = config . get ( \"optima\" , None ) if optima : optima = pd . DataFrame ( ** config [ \"optima\" ]) return Problem ( inputs = config [ \"inputs\" ], outputs = outputs , objectives = objectives , constraints = config . get ( \"constraints\" , None ), output_constraints = output_constraints , models = models , data = data , optima = optima , name = config . get ( \"name\" , None ), )","title":"from_config()"},{"location":"ref-problem/#opti.problem.Problem.from_json","text":"Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem . from_config ( config )","title":"from_json()"},{"location":"ref-problem/#opti.problem.Problem.get_X","text":"Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values","title":"get_X()"},{"location":"ref-problem/#opti.problem.Problem.get_XY","text":"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y","title":"get_XY()"},{"location":"ref-problem/#opti.problem.Problem.get_X_bounds","text":"Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi","title":"get_X_bounds()"},{"location":"ref-problem/#opti.problem.Problem.get_Y","text":"Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values","title":"get_Y()"},{"location":"ref-problem/#opti.problem.Problem.get_data","text":"Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data","title":"get_data()"},{"location":"ref-problem/#opti.problem.Problem.sample_inputs","text":"Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints )","title":"sample_inputs()"},{"location":"ref-problem/#opti.problem.Problem.set_data","text":"Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data","title":"set_data()"},{"location":"ref-problem/#opti.problem.Problem.set_optima","text":"Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima","title":"set_optima()"},{"location":"ref-problem/#opti.problem.Problem.to_config","text":"Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> Dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config","title":"to_config()"},{"location":"ref-problem/#opti.problem.Problem.to_json","text":"Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , indent = 2 ) outfile . write ( b . encode ( \"utf-8\" ))","title":"to_json()"},{"location":"ref-problem/#opti.problem.read_json","text":"Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"read_json()"},{"location":"ref-problems/","text":"Test Problems baking Bread ( Problem ) Dummy problem with mixed inputs and outputs. benchmark Daechert1 ( Problem ) Problem with a non-convex Pareto front. The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. Daechert2 ( Problem ) Unconstrained problem with a Pareto front resembling a comet. minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. Daechert3 ( Problem ) Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. Hyperellipsoid ( Problem ) Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. required a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. required Qapi1 ( Problem ) Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. detergent Detergent ( Problem ) Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. Detergent_NChooseKConstraint ( Problem ) Variant of the Detergent problem with an n-choose-k constraint Detergent_OutputConstraint ( Problem ) Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable) mixed Mixed variables single and multi-objective test problems. DiscreteFuelInjector ( Problem ) Fuel injector test problem, modified to contain an integer variable. !!! see Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 !!! properties 4 objectives, mixed variables, unconstrained DiscreteVLMOP2 ( Problem ) VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. !!! see Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 !!! properties bi-objective, mixed variables, unconstrained multi_fidelity create_multi_fidelity ( problem , fidelities , predicates , name = None ) Based on a given problem, this creates a new problem with different fidelities. Internally, a counter is increased on every call. Parameters: Name Type Description Default problem Problem base problem required fidelities Sequence[Callable[[numpy.ndarray], numpy.ndarray]] callables that evaluate the problem with different fidelities required predicates Sequence[Callable[[int, numpy.ndarray], bool]] callables that decide which fidelity is active at the current call depending on the counter and the inputs required name Optional[str] name of the new multi-fidelity instance None Returns: new problem with different fidelities Examples: We apply two Gaussians as two fidelities to the zdt1-problem. zdt1 = opti.problems.ZDT1(n_inputs=n_inputs) first_fidelity = noisify_problem_with_gaussian(zdt1, mu=0, sigma=0.01) second_fidelity = noisify_problem_with_gaussian(zdt1, mu=0.1, sigma=0.5) def pred_second(counter, X): return counter % 3 == 0 def pred_first(counter, X): return not pred_second(counter, _) mf_zdt = create_multi_fidelity( zdt1, [first_fidelity.f, second_fidelity.f], [pred_first, pred_second] ) [...] when calling subsequently we obtain mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity ... Source code in opti/problems/multi_fidelity.py def create_multi_fidelity ( problem : Problem , fidelities : Sequence [ Callable [[ np . ndarray ], np . ndarray ]], predicates : Sequence [ Callable [[ int , np . ndarray ], bool ]], name : Optional [ str ] = None , ): \"\"\" Based on a given problem, this creates a new problem with different fidelities. Internally, a counter is increased on every call. Args: problem: base problem fidelities: callables that evaluate the problem with different fidelities predicates: callables that decide which fidelity is active at the current call depending on the counter and the inputs name: name of the new multi-fidelity instance Returns: new problem with different fidelities Example: # We apply two Gaussians as two fidelities to the zdt1-problem. zdt1 = opti.problems.ZDT1(n_inputs=n_inputs) first_fidelity = noisify_problem_with_gaussian(zdt1, mu=0, sigma=0.01) second_fidelity = noisify_problem_with_gaussian(zdt1, mu=0.1, sigma=0.5) def pred_second(counter, X): return counter % 3 == 0 def pred_first(counter, X): return not pred_second(counter, _) mf_zdt = create_multi_fidelity( zdt1, [first_fidelity.f, second_fidelity.f], [pred_first, pred_second] ) [...] # when calling subsequently we obtain mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity ... \"\"\" class MultiFidelityF : def __init__ ( self ): self . counter = 0 def __call__ ( self , X ): self . counter += 1 for fidelity , pred in zip ( fidelities , predicates ): if pred ( self . counter - 1 , X ): return fidelity ( X ) raise ValueError ( \"no predicate returned true\" ) return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = MultiFidelityF (), data = None , name = problem . name if name is None else name , ) noisify NoiseType ( Enum ) An enumeration. noisify_problem ( problem , noisifiers , name = None ) Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers Union[Callable, List[Callable]] function or list of functions that add noise to the outputs required name Optional[str] name of the new problem None Returns: new problem with noise on the output Source code in opti/problems/noisify.py def noisify_problem ( problem : Problem , noisifiers : Union [ Callable , List [ Callable ]], name : Optional [ str ] = None , ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: function or list of functions that add noise to the outputs name: name of the new problem Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): nonlocal noisifiers if isinstance ( noisifiers , Callable ): noisifiers = [ noisifiers ] * problem . n_outputs Y = problem . f ( X ) return _add_noise_to_data ( Y , noisifiers , problem . outputs ) if problem . data is not None : noisy_Y = _add_noise_to_data ( problem . data [ problem . outputs . names ] . values , noisifiers , problem . outputs ) data = pd . concat ( [ problem . data [ problem . inputs . names ], pd . DataFrame ( columns = problem . outputs . names , index = problem . data . index , data = noisy_Y , ), ], axis = 1 , ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name if name is None else name , ) noisify_problem_with_gaussian ( problem , mu = 0 , sigma = 0.05 ) Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/problems/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" rv_kwargs = { \"loc\" : mu , \"scale\" : sigma } noise_type = NoiseType . ADDITIVE return noisify_problem_with_scipy_stats ( problem , stats . norm , rv_kwargs , noise_type ) noisify_problem_with_scipy_stats ( problem , random_variables , rv_kwargs = None , noise_types =< NoiseType . ADDITIVE : 1 > , name = None ) Add additive or multiplicative noise drawn from a distribution in scipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html Parameters: Name Type Description Default problem Problem problem instance where we want to add noise to required random_variables Union[list, Any] list of instances or instance of random variables, e.g., [scipy.stats.beta, scipy.stats.cauchy], each element of the list is applied to the corresponding data column required rv_kwargs Union[NoneType, dict, List[dict]] keyword arguments for the random variables, type must correspond to the on of random_variables if given. None noise_types Union[List[opti.problems.noisify.NoiseType], opti.problems.noisify.NoiseType] list of the same length as random_variables or single value, decides whether the noise is added or multiplied <NoiseType.ADDITIVE: 1> name Optional[str] name of the noisy problem None Returns: noisy problem instance Examples: To see how to obtain noisy flow-reactor-problems check the following examples. 1.) Additive Gaussian noise for all output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm noisy_flow = noisify_problem_with_scipy_stats(flow, rv) 2.) Additive Gaussian noise for all output columns with non-standard parameters from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm rv_kwargs = {\"loc\": 0.5, \"scale\": 0.3} noisy_flow = noisify_problem_with_scipy_stats(flow, rv, rv_kwargs=rv_kwargs) 3.) Different noise distributions and types for the different output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rvs = [stats.truncnorm, stats.norm, stats.gamma] rv_kwargs = [{\"a\": 0.1, \"b\": 2}, {\"loc\": 0.5, \"scale\": 0.3}, {\"a\": 1.99}] noise_types = [NoiseType.ADDITIVE, NoiseType.ADDITIVE, NoiseType.MULTIPLICATIVE] noisy_flow = noisify_problem_with_scipy_stats( flow, rvs, rv_kwargs=rv_kwargs, noise_types=noise_types ) Source code in opti/problems/noisify.py def noisify_problem_with_scipy_stats ( problem : Problem , random_variables : Union [ list , Any ], rv_kwargs : Union [ None , dict , List [ dict ]] = None , noise_types : Union [ List [ NoiseType ], NoiseType ] = NoiseType . ADDITIVE , name : Optional [ str ] = None , ) -> Problem : \"\"\" Add additive or multiplicative noise drawn from a distribution in scipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html Args: problem: problem instance where we want to add noise to random_variables: list of instances or instance of random variables, e.g., [scipy.stats.beta, scipy.stats.cauchy], each element of the list is applied to the corresponding data column rv_kwargs: keyword arguments for the random variables, type must correspond to the on of random_variables if given. noise_types: list of the same length as random_variables or single value, decides whether the noise is added or multiplied name: name of the noisy problem Returns: noisy problem instance Example: To see how to obtain noisy flow-reactor-problems check the following examples. 1.) Additive Gaussian noise for all output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm noisy_flow = noisify_problem_with_scipy_stats(flow, rv) 2.) Additive Gaussian noise for all output columns with non-standard parameters from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm rv_kwargs = {\"loc\": 0.5, \"scale\": 0.3} noisy_flow = noisify_problem_with_scipy_stats(flow, rv, rv_kwargs=rv_kwargs) 3.) Different noise distributions and types for the different output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rvs = [stats.truncnorm, stats.norm, stats.gamma] rv_kwargs = [{\"a\": 0.1, \"b\": 2}, {\"loc\": 0.5, \"scale\": 0.3}, {\"a\": 1.99}] noise_types = [NoiseType.ADDITIVE, NoiseType.ADDITIVE, NoiseType.MULTIPLICATIVE] noisy_flow = noisify_problem_with_scipy_stats( flow, rvs, rv_kwargs=rv_kwargs, noise_types=noise_types ) \"\"\" # listify single element inputs if not isinstance ( random_variables , list ): random_variables = [ random_variables ] * problem . n_outputs rv_kwargs = [ rv_kwargs ] if rv_kwargs is not None else [{}] rv_kwargs = rv_kwargs * problem . n_outputs if not isinstance ( noise_types , list ): noise_types = [ noise_types ] * problem . n_outputs ops = { NoiseType . ADDITIVE : lambda x , y : x + y , NoiseType . MULTIPLICATIVE : lambda x , y : x * y , } op_names = { NoiseType . ADDITIVE : \"add\" , NoiseType . MULTIPLICATIVE : \"mul\" , } def apply_noise ( data_column , noise_type , random_variable , rv_kwargs ): sampled_noise = random_variable . rvs ( size = data_column . size , ** rv_kwargs ) return ops [ noise_type ]( data_column , sampled_noise ) noisifiers = [ partial ( apply_noise , noise_type = nt , random_variable = rv , rv_kwargs = rvk ) for nt , rv , rvk in zip ( noise_types , random_variables , rv_kwargs ) ] if name is None : rv_type_names = [ f \" { rv . name } _ { op_names [ nt ] } \" for rv , nt in zip ( random_variables , noise_types ) ] name = f \" { problem . name } with rvs { ',' . join ( rv_type_names ) } \" return noisify_problem ( problem , noisifiers , name = name ) single Zakharov_Categorical ( Problem ) Variant of the Zakharov problem with one categorical input Zakharov_Constrained ( Problem ) Variant of the Zakharov problem with one linear constraint Zakharov_NChooseKConstraint ( Problem ) Variant of the Zakharov problem with an n-choose-k constraint univariate Simple 1D problems for visually assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 ) zdt ZDT problem test suite. Reference: Zitzler, Eckart, Kalyanmoy Deb, and Lothar Thiele. \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation 8.2 (2000): 173-195. doi: 10.1.1.30.5848 Adapted from https://github.com/msu-coinlab/pymoo/blob/master/pymoo/problems/multi/zdt.py","title":"Test Problems"},{"location":"ref-problems/#test-problems","text":"","title":"Test Problems"},{"location":"ref-problems/#opti.problems.baking","text":"","title":"baking"},{"location":"ref-problems/#opti.problems.baking.Bread","text":"Dummy problem with mixed inputs and outputs.","title":"Bread"},{"location":"ref-problems/#opti.problems.benchmark","text":"","title":"benchmark"},{"location":"ref-problems/#opti.problems.benchmark.Daechert1","text":"Problem with a non-convex Pareto front. The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44].","title":"Daechert1"},{"location":"ref-problems/#opti.problems.benchmark.Daechert2","text":"Unconstrained problem with a Pareto front resembling a comet. minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5].","title":"Daechert2"},{"location":"ref-problems/#opti.problems.benchmark.Daechert3","text":"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6].","title":"Daechert3"},{"location":"ref-problems/#opti.problems.benchmark.Hyperellipsoid","text":"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. required a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. required","title":"Hyperellipsoid"},{"location":"ref-problems/#opti.problems.benchmark.Qapi1","text":"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8].","title":"Qapi1"},{"location":"ref-problems/#opti.problems.detergent","text":"","title":"detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent","text":"Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters.","title":"Detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent_NChooseKConstraint","text":"Variant of the Detergent problem with an n-choose-k constraint","title":"Detergent_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.detergent.Detergent_OutputConstraint","text":"Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable)","title":"Detergent_OutputConstraint"},{"location":"ref-problems/#opti.problems.mixed","text":"Mixed variables single and multi-objective test problems.","title":"mixed"},{"location":"ref-problems/#opti.problems.mixed.DiscreteFuelInjector","text":"Fuel injector test problem, modified to contain an integer variable. !!! see Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 !!! properties 4 objectives, mixed variables, unconstrained","title":"DiscreteFuelInjector"},{"location":"ref-problems/#opti.problems.mixed.DiscreteVLMOP2","text":"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. !!! see Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 !!! properties bi-objective, mixed variables, unconstrained","title":"DiscreteVLMOP2"},{"location":"ref-problems/#opti.problems.multi_fidelity","text":"","title":"multi_fidelity"},{"location":"ref-problems/#opti.problems.multi_fidelity.create_multi_fidelity","text":"Based on a given problem, this creates a new problem with different fidelities. Internally, a counter is increased on every call. Parameters: Name Type Description Default problem Problem base problem required fidelities Sequence[Callable[[numpy.ndarray], numpy.ndarray]] callables that evaluate the problem with different fidelities required predicates Sequence[Callable[[int, numpy.ndarray], bool]] callables that decide which fidelity is active at the current call depending on the counter and the inputs required name Optional[str] name of the new multi-fidelity instance None Returns: new problem with different fidelities Examples:","title":"create_multi_fidelity()"},{"location":"ref-problems/#opti.problems.multi_fidelity.create_multi_fidelity--we-apply-two-gaussians-as-two-fidelities-to-the-zdt1-problem","text":"zdt1 = opti.problems.ZDT1(n_inputs=n_inputs) first_fidelity = noisify_problem_with_gaussian(zdt1, mu=0, sigma=0.01) second_fidelity = noisify_problem_with_gaussian(zdt1, mu=0.1, sigma=0.5) def pred_second(counter, X): return counter % 3 == 0 def pred_first(counter, X): return not pred_second(counter, _) mf_zdt = create_multi_fidelity( zdt1, [first_fidelity.f, second_fidelity.f], [pred_first, pred_second] ) [...]","title":"We apply two Gaussians as two fidelities to the zdt1-problem."},{"location":"ref-problems/#opti.problems.multi_fidelity.create_multi_fidelity--when-calling-subsequently-we-obtain","text":"mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity ... Source code in opti/problems/multi_fidelity.py def create_multi_fidelity ( problem : Problem , fidelities : Sequence [ Callable [[ np . ndarray ], np . ndarray ]], predicates : Sequence [ Callable [[ int , np . ndarray ], bool ]], name : Optional [ str ] = None , ): \"\"\" Based on a given problem, this creates a new problem with different fidelities. Internally, a counter is increased on every call. Args: problem: base problem fidelities: callables that evaluate the problem with different fidelities predicates: callables that decide which fidelity is active at the current call depending on the counter and the inputs name: name of the new multi-fidelity instance Returns: new problem with different fidelities Example: # We apply two Gaussians as two fidelities to the zdt1-problem. zdt1 = opti.problems.ZDT1(n_inputs=n_inputs) first_fidelity = noisify_problem_with_gaussian(zdt1, mu=0, sigma=0.01) second_fidelity = noisify_problem_with_gaussian(zdt1, mu=0.1, sigma=0.5) def pred_second(counter, X): return counter % 3 == 0 def pred_first(counter, X): return not pred_second(counter, _) mf_zdt = create_multi_fidelity( zdt1, [first_fidelity.f, second_fidelity.f], [pred_first, pred_second] ) [...] # when calling subsequently we obtain mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # first fidelity mf_zdt.f(X) # second fidelity mf_zdt.f(X) # first fidelity ... \"\"\" class MultiFidelityF : def __init__ ( self ): self . counter = 0 def __call__ ( self , X ): self . counter += 1 for fidelity , pred in zip ( fidelities , predicates ): if pred ( self . counter - 1 , X ): return fidelity ( X ) raise ValueError ( \"no predicate returned true\" ) return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = MultiFidelityF (), data = None , name = problem . name if name is None else name , )","title":"when calling subsequently we obtain"},{"location":"ref-problems/#opti.problems.noisify","text":"","title":"noisify"},{"location":"ref-problems/#opti.problems.noisify.NoiseType","text":"An enumeration.","title":"NoiseType"},{"location":"ref-problems/#opti.problems.noisify.noisify_problem","text":"Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers Union[Callable, List[Callable]] function or list of functions that add noise to the outputs required name Optional[str] name of the new problem None Returns: new problem with noise on the output Source code in opti/problems/noisify.py def noisify_problem ( problem : Problem , noisifiers : Union [ Callable , List [ Callable ]], name : Optional [ str ] = None , ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: function or list of functions that add noise to the outputs name: name of the new problem Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): nonlocal noisifiers if isinstance ( noisifiers , Callable ): noisifiers = [ noisifiers ] * problem . n_outputs Y = problem . f ( X ) return _add_noise_to_data ( Y , noisifiers , problem . outputs ) if problem . data is not None : noisy_Y = _add_noise_to_data ( problem . data [ problem . outputs . names ] . values , noisifiers , problem . outputs ) data = pd . concat ( [ problem . data [ problem . inputs . names ], pd . DataFrame ( columns = problem . outputs . names , index = problem . data . index , data = noisy_Y , ), ], axis = 1 , ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name if name is None else name , )","title":"noisify_problem()"},{"location":"ref-problems/#opti.problems.noisify.noisify_problem_with_gaussian","text":"Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/problems/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" rv_kwargs = { \"loc\" : mu , \"scale\" : sigma } noise_type = NoiseType . ADDITIVE return noisify_problem_with_scipy_stats ( problem , stats . norm , rv_kwargs , noise_type )","title":"noisify_problem_with_gaussian()"},{"location":"ref-problems/#opti.problems.noisify.noisify_problem_with_scipy_stats","text":"Add additive or multiplicative noise drawn from a distribution in scipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html Parameters: Name Type Description Default problem Problem problem instance where we want to add noise to required random_variables Union[list, Any] list of instances or instance of random variables, e.g., [scipy.stats.beta, scipy.stats.cauchy], each element of the list is applied to the corresponding data column required rv_kwargs Union[NoneType, dict, List[dict]] keyword arguments for the random variables, type must correspond to the on of random_variables if given. None noise_types Union[List[opti.problems.noisify.NoiseType], opti.problems.noisify.NoiseType] list of the same length as random_variables or single value, decides whether the noise is added or multiplied <NoiseType.ADDITIVE: 1> name Optional[str] name of the noisy problem None Returns: noisy problem instance Examples: To see how to obtain noisy flow-reactor-problems check the following examples. 1.) Additive Gaussian noise for all output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm noisy_flow = noisify_problem_with_scipy_stats(flow, rv) 2.) Additive Gaussian noise for all output columns with non-standard parameters from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm rv_kwargs = {\"loc\": 0.5, \"scale\": 0.3} noisy_flow = noisify_problem_with_scipy_stats(flow, rv, rv_kwargs=rv_kwargs) 3.) Different noise distributions and types for the different output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rvs = [stats.truncnorm, stats.norm, stats.gamma] rv_kwargs = [{\"a\": 0.1, \"b\": 2}, {\"loc\": 0.5, \"scale\": 0.3}, {\"a\": 1.99}] noise_types = [NoiseType.ADDITIVE, NoiseType.ADDITIVE, NoiseType.MULTIPLICATIVE] noisy_flow = noisify_problem_with_scipy_stats( flow, rvs, rv_kwargs=rv_kwargs, noise_types=noise_types ) Source code in opti/problems/noisify.py def noisify_problem_with_scipy_stats ( problem : Problem , random_variables : Union [ list , Any ], rv_kwargs : Union [ None , dict , List [ dict ]] = None , noise_types : Union [ List [ NoiseType ], NoiseType ] = NoiseType . ADDITIVE , name : Optional [ str ] = None , ) -> Problem : \"\"\" Add additive or multiplicative noise drawn from a distribution in scipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html Args: problem: problem instance where we want to add noise to random_variables: list of instances or instance of random variables, e.g., [scipy.stats.beta, scipy.stats.cauchy], each element of the list is applied to the corresponding data column rv_kwargs: keyword arguments for the random variables, type must correspond to the on of random_variables if given. noise_types: list of the same length as random_variables or single value, decides whether the noise is added or multiplied name: name of the noisy problem Returns: noisy problem instance Example: To see how to obtain noisy flow-reactor-problems check the following examples. 1.) Additive Gaussian noise for all output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm noisy_flow = noisify_problem_with_scipy_stats(flow, rv) 2.) Additive Gaussian noise for all output columns with non-standard parameters from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rv = stats.norm rv_kwargs = {\"loc\": 0.5, \"scale\": 0.3} noisy_flow = noisify_problem_with_scipy_stats(flow, rv, rv_kwargs=rv_kwargs) 3.) Different noise distributions and types for the different output columns from scipy import stats from opti.problems.flow_reactor_unconstrained import get_problem_pareto flow = get_problem_pareto() rvs = [stats.truncnorm, stats.norm, stats.gamma] rv_kwargs = [{\"a\": 0.1, \"b\": 2}, {\"loc\": 0.5, \"scale\": 0.3}, {\"a\": 1.99}] noise_types = [NoiseType.ADDITIVE, NoiseType.ADDITIVE, NoiseType.MULTIPLICATIVE] noisy_flow = noisify_problem_with_scipy_stats( flow, rvs, rv_kwargs=rv_kwargs, noise_types=noise_types ) \"\"\" # listify single element inputs if not isinstance ( random_variables , list ): random_variables = [ random_variables ] * problem . n_outputs rv_kwargs = [ rv_kwargs ] if rv_kwargs is not None else [{}] rv_kwargs = rv_kwargs * problem . n_outputs if not isinstance ( noise_types , list ): noise_types = [ noise_types ] * problem . n_outputs ops = { NoiseType . ADDITIVE : lambda x , y : x + y , NoiseType . MULTIPLICATIVE : lambda x , y : x * y , } op_names = { NoiseType . ADDITIVE : \"add\" , NoiseType . MULTIPLICATIVE : \"mul\" , } def apply_noise ( data_column , noise_type , random_variable , rv_kwargs ): sampled_noise = random_variable . rvs ( size = data_column . size , ** rv_kwargs ) return ops [ noise_type ]( data_column , sampled_noise ) noisifiers = [ partial ( apply_noise , noise_type = nt , random_variable = rv , rv_kwargs = rvk ) for nt , rv , rvk in zip ( noise_types , random_variables , rv_kwargs ) ] if name is None : rv_type_names = [ f \" { rv . name } _ { op_names [ nt ] } \" for rv , nt in zip ( random_variables , noise_types ) ] name = f \" { problem . name } with rvs { ',' . join ( rv_type_names ) } \" return noisify_problem ( problem , noisifiers , name = name )","title":"noisify_problem_with_scipy_stats()"},{"location":"ref-problems/#opti.problems.single","text":"","title":"single"},{"location":"ref-problems/#opti.problems.single.Zakharov_Categorical","text":"Variant of the Zakharov problem with one categorical input","title":"Zakharov_Categorical"},{"location":"ref-problems/#opti.problems.single.Zakharov_Constrained","text":"Variant of the Zakharov problem with one linear constraint","title":"Zakharov_Constrained"},{"location":"ref-problems/#opti.problems.single.Zakharov_NChooseKConstraint","text":"Variant of the Zakharov problem with an n-choose-k constraint","title":"Zakharov_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.univariate","text":"Simple 1D problems for visually assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 )","title":"univariate"},{"location":"ref-problems/#opti.problems.zdt","text":"ZDT problem test suite. Reference: Zitzler, Eckart, Kalyanmoy Deb, and Lothar Thiele. \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation 8.2 (2000): 173-195. doi: 10.1.1.30.5848 Adapted from https://github.com/msu-coinlab/pymoo/blob/master/pymoo/problems/multi/zdt.py","title":"zdt"},{"location":"ref-sampling/","text":"Sampling base apply_nchoosek ( samples , constraint ) Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0 constrained_sampling ( n_samples , parameters , constraints ) Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples rejection_sampling ( n_samples , parameters , constraints , max_iters = 1000 ) Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ] sobol_sampling ( n_samples , parameters ) Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 ) split_nchoosek ( constraints ) Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints polytope This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module. References .. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html polytope_sampling ( n_samples , parameters , constraints , thin = 100 ) Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X ) simplex grid ( dimension , levels ) Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) !!! references Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n sample ( dimension , n_samples = 1 ) Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T sphere sample ( dimension , n_samples = 1 , positive = False ) Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"Sampling"},{"location":"ref-sampling/#sampling","text":"","title":"Sampling"},{"location":"ref-sampling/#opti.sampling.base","text":"","title":"base"},{"location":"ref-sampling/#opti.sampling.base.apply_nchoosek","text":"Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0","title":"apply_nchoosek()"},{"location":"ref-sampling/#opti.sampling.base.constrained_sampling","text":"Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples","title":"constrained_sampling()"},{"location":"ref-sampling/#opti.sampling.base.rejection_sampling","text":"Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ]","title":"rejection_sampling()"},{"location":"ref-sampling/#opti.sampling.base.sobol_sampling","text":"Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 )","title":"sobol_sampling()"},{"location":"ref-sampling/#opti.sampling.base.split_nchoosek","text":"Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints","title":"split_nchoosek()"},{"location":"ref-sampling/#opti.sampling.polytope","text":"This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module.","title":"polytope"},{"location":"ref-sampling/#opti.sampling.polytope--references","text":".. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html","title":"References"},{"location":"ref-sampling/#opti.sampling.polytope.polytope_sampling","text":"Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X )","title":"polytope_sampling()"},{"location":"ref-sampling/#opti.sampling.simplex","text":"","title":"simplex"},{"location":"ref-sampling/#opti.sampling.simplex.grid","text":"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) !!! references Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n","title":"grid()"},{"location":"ref-sampling/#opti.sampling.simplex.sample","text":"Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T","title":"sample()"},{"location":"ref-sampling/#opti.sampling.sphere","text":"","title":"sphere"},{"location":"ref-sampling/#opti.sampling.sphere.sample","text":"Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"sample()"},{"location":"ref-tools/","text":"Tools modde MipFile File reader for MODDE investigation files (.mip) __init__ ( self , filename ) special Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () read_modde ( filepath ) Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data )","title":"Tools"},{"location":"ref-tools/#tools","text":"","title":"Tools"},{"location":"ref-tools/#opti.tools.modde","text":"","title":"modde"},{"location":"ref-tools/#opti.tools.modde.MipFile","text":"File reader for MODDE investigation files (.mip)","title":"MipFile"},{"location":"ref-tools/#opti.tools.modde.MipFile.__init__","text":"Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data ()","title":"__init__()"},{"location":"ref-tools/#opti.tools.modde.read_modde","text":"Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data )","title":"read_modde()"}]}